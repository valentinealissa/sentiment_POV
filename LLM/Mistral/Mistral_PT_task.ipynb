{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/hpc/users/valena17/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34756edd9c3841c5b55e875121998b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8183c81bbc45389a77d497ef628377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b5a214df5c4592b68e0f15e13a0e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/users/valena17/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae32cd6b4df4bdfa7ebeb216d7bc341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f653a0c3288d4b5580769b186ebeb942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from statistics import mean\n",
    "import torch, time, json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# load a base model and tokenizer\n",
    "access_token = #your token here\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", token=access_token, cache_dir='./')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", token=access_token, cache_dir='./',\n",
    "                                             device_map={\"\": accelerator.process_index},\n",
    "                                             torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_garbage(s):\n",
    "    s = s[next(idx for idx, c in enumerate(s) if c in \"{[\"):]\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return json.loads(s[:e.pos])\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, references, labels=None, pos_label=1, average=\"weighted\", sample_weight=None, zero_division='warn'):\n",
    "        f1 = f1_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight\n",
    "        )\n",
    "        p = precision_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight,\n",
    "            zero_division=zero_division\n",
    "        )\n",
    "        r = recall_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight,\n",
    "            zero_division=zero_division\n",
    "        )\n",
    "        c = classification_report(\n",
    "            references, predictions, labels=labels\n",
    "        )\n",
    "        print(c)\n",
    "        return {\"f1\": float(f1) if f1.size == 1 else f1,\n",
    "                \"precision\": float(p) if p.size == 1 else p,\n",
    "                \"recall\": float(r) if r.size == 1 else r}\n",
    "\n",
    "def train_sentence_selection(df, n_neutral, n_positive, n_negative, label, seed):\n",
    "    # saving column name given PT or MD label\n",
    "    label = f\"{label}_label\"\n",
    "    # randomly selecting 1-2 sentences per label\n",
    "    neutral_sentences = df[df[label] == \"neutral\"].sample(n_neutral, replace=False, random_state=seed)\n",
    "    positive_sentences = df[df[label] == \"positive\"].sample(n_positive, replace=False, random_state=seed)\n",
    "    negative_sentences = df[df[label] == \"negative\"].sample(n_negative, replace=False, random_state=seed)\n",
    "    all_sentences = pd.concat([neutral_sentences,\n",
    "                               positive_sentences,\n",
    "                               negative_sentences], ignore_index = True)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 20.0, time 6.015211343765259, total tokens 123, total prompts 1\n",
      "{\n",
      "\"0\": \"neutral\",\n",
      "\"1\": \"positive\",\n",
      "\"2\": \"negative\",\n",
      "\"3\": \"negative\",\n",
      "\"4\": \"negative\",\n",
      "\"5\": \"negative\",\n",
      "\"6\": \"neutral\",\n",
      "\"7\": \"positive\",\n",
      "\"8\": \"positive\",\n",
      "\"9\": \"neutral\",\n",
      "\"10\": \"negative\",\n",
      "\"11\": \"neutral\",\n",
      "\"12\": \"neutral\",\n",
      "\"13\": \"negative\",\n",
      "\"14\": \"neutral\"\n",
      "}</s>\n",
      "           0\n",
      "0    neutral\n",
      "1   positive\n",
      "2   negative\n",
      "3   negative\n",
      "4   negative\n",
      "5   negative\n",
      "6    neutral\n",
      "7   positive\n",
      "8   positive\n",
      "9    neutral\n",
      "10  negative\n",
      "11   neutral\n",
      "12   neutral\n",
      "13  negative\n",
      "14   neutral \n",
      " 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80         9\n",
      "     neutral       0.50      0.75      0.60         4\n",
      "    positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.72      0.81      0.73        15\n",
      "weighted avg       0.82      0.73      0.75        15\n",
      "\n",
      "{'f1': 0.7466666666666666, 'precision': 0.8222222222222223, 'recall': 0.7333333333333333}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## zero shot\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "\n",
    "dataset = \"test\"\n",
    "\n",
    "# test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/test_{dataset}.csv\")\n",
    "test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/validation_sentences.csv\")\n",
    "# test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/MD_PT_disagreement.csv\")\n",
    "\n",
    "# converting test sentences to json format\n",
    "json_test_sentences = test[\"language\"].to_json()\n",
    "\n",
    "# creating context prompt\n",
    "\n",
    "USER_PROMPT_1 = f\"\"\"\n",
    "Doctors write lots of clinical notes about you.\n",
    "Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\n",
    "For each sentence, how do you feel reading this description of you?\n",
    "Answer the question by assigning a sentiment score of negative, neutral, or positive for each sentence.\n",
    "Output only your sentiment scores in JSON format:\n",
    "{json_test_sentences}\n",
    "\"\"\"\n",
    "    \n",
    "PROMPT = [f\"\"\"<s>[INST]{USER_PROMPT_1}[/INST]\"\"\"]\n",
    "    \n",
    "with accelerator.split_between_processes(PROMPT) as prompts:\n",
    "        # store output of generations in dict\n",
    "    results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=600, do_sample=True, temperature=0.001)[0]\n",
    "\n",
    "        # remove prompt from output\n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "        results[\"num_tokens\"] += len(output_tokenized)\n",
    "\n",
    "        results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    timediff=time.time()-start\n",
    "    num_tokens=sum([r[\"num_tokens\"] for r in results_gathered ])\n",
    "\n",
    "    print(f\"tokens/sec: {num_tokens//timediff}, time {timediff}, total tokens {num_tokens}, total prompts {len(PROMPT)}\")\n",
    "\n",
    "for o in results_gathered[0]['outputs']:\n",
    "    print(o)\n",
    "    \n",
    "    # running chat completion\n",
    "output = parse_json_garbage(o.lower())\n",
    "\n",
    "try:\n",
    "    output = pd.DataFrame.from_dict(output).transpose()\n",
    "except ValueError:\n",
    "    output = pd.DataFrame.from_dict(output, orient='index')\n",
    "column_name = list(output.columns.values)\n",
    "column_name = column_name[0]\n",
    "print(output, '\\n', column_name)\n",
    "\n",
    "# computing metrics\n",
    "results = compute_metrics(output, test[\"PT_label\"])\n",
    "print(results)\n",
    "\n",
    "error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], output[column_name]], axis=1)\n",
    "error_analysis = error_analysis.rename(columns={column_name:\"pred\", \"PT_label\":\"true\"})\n",
    "mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "error_analysis = error_analysis[~ mask]\n",
    "error_analysis.to_csv(f'error_analysis_zero_shot_{dataset}.csv', index=False)\n",
    "print('-' * 100)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 22.0, time 5.248542308807373, total tokens 117, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      " \"1\": \"neutral\",\n",
      " \"2\": \"negative\",\n",
      " \"3\": \"negative\",\n",
      " \"4\": \"negative\",\n",
      " \"5\": \"negative\",\n",
      " \"6\": \"neutral\",\n",
      " \"7\": \"positive\",\n",
      " \"8\": \"positive\",\n",
      " \"9\": \"negative\",\n",
      " \"10\": \"negative\",\n",
      " \"11\": \"neutral\",\n",
      " \"12\": \"negative\",\n",
      " \"13\": \"negative\",\n",
      " \"14\": \"negative\"\n",
      "}</s>\n",
      "negative\n",
      " neutral\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      " neutral\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      " neutral\n",
      "negative\n",
      "negative\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "## ICL performance on test sentences\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "\n",
    "dataset = \"60\"\n",
    "\n",
    "train = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/PT_task/Mistral/best_CLI_sentences_{dataset}.csv\")\n",
    "test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/validation_sentences.csv\")\n",
    "\n",
    "#context sentences in json format\n",
    "json_train_sentences = train[\"language\"].to_json()\n",
    "json_train_labels = train[\"PT_label\"].to_json()\n",
    "\n",
    "# converting test sentences to json format\n",
    "json_test_sentences = test[\"language\"].to_json()\n",
    "\n",
    "# creating context prompt\n",
    "USER_PROMPT_1 = f\"\"\"\n",
    "Doctors write lots of clinical notes about you.\n",
    "Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\n",
    "For each sentence, how do you feel reading this description of you?\n",
    "Answer the question by assigning a sentiment score of negative, neutral, or positive for each sentence.\n",
    "Output your sentiment scores in valid JSON format for these example sentences:\n",
    "{json_train_sentences}\n",
    "\"\"\"\n",
    "    \n",
    "ASSISTANT_PROMPT = f\"{json_train_labels}\"\n",
    "\n",
    "USER_PROMPT_2 = f\"\"\"\n",
    "Complete the same task with each of these sentences and output your sentiment scores (negative, neutral, or positive) in JSON format:\n",
    "{json_test_sentences} \n",
    "\"\"\"\n",
    "\n",
    "PROMPT = [f\"\"\"\n",
    "<s>[INST]{USER_PROMPT_1}[/INST]\n",
    "{ASSISTANT_PROMPT}</s>\n",
    "[INST]{USER_PROMPT_2}[/INST]\n",
    "\"\"\"]\n",
    "    \n",
    "with accelerator.split_between_processes(PROMPT) as prompts:\n",
    "        # store output of generations in dict\n",
    "    results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=800, do_sample=True, temperature=0.001)[0]\n",
    "\n",
    "        # remove prompt from output\n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "        results[\"num_tokens\"] += len(output_tokenized)\n",
    "\n",
    "        results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    timediff=time.time()-start\n",
    "    num_tokens=sum([r[\"num_tokens\"] for r in results_gathered ])\n",
    "\n",
    "    print(f\"tokens/sec: {num_tokens//timediff}, time {timediff}, total tokens {num_tokens}, total prompts {len(PROMPT)}\")\n",
    "\n",
    "for o in results_gathered[0]['outputs']:\n",
    "    print(o)\n",
    "    \n",
    "    # running chat completion\n",
    "output = parse_json_garbage(o.lower())\n",
    "\n",
    "try:\n",
    "    output = pd.DataFrame.from_dict(output).transpose()\n",
    "except ValueError:\n",
    "    output = pd.DataFrame.from_dict(output, orient='index')\n",
    "column_name = list(output.columns.values)\n",
    "column_name = column_name[0]\n",
    "# print(output, '\\n', column_name)\n",
    "\n",
    "    \n",
    "#     # computing metrics\n",
    "# try: \n",
    "#     results = compute_metrics(output, test[\"PT_label\"])\n",
    "# except ValueError:\n",
    "#     results = compute_metrics(output.transpose(), test[\"PT_label\"])\n",
    "# print(results)\n",
    "\n",
    "# error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], output[column_name]], axis=1)\n",
    "# error_analysis = error_analysis.rename(columns={column_name:\"pred\", \"PT_label\":\"true\"})\n",
    "# mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "# error_analysis = error_analysis[~ mask]\n",
    "# error_analysis.to_csv(f'error_analysis_test_{dataset}.csv', index=False)\n",
    "# print('-' * 100)\n",
    "# print('\\n\\n')\n",
    "print(output[column_name].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 0 negative, 0 neutral, 0 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 22.0, time 3.7476720809936523, total tokens 86, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral to negative\",\n",
      "      \"9\": \"neutral\"}</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3             negative\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9              neutral \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       1.00      0.83      0.91         6\n",
      "            neutral       1.00      1.00      1.00         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.90        10\n",
      "          macro avg       0.75      0.71      0.73        10\n",
      "       weighted avg       1.00      0.90      0.95        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 0 negative, 0 neutral, 1 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 11.0, time 7.7939698696136475, total tokens 90, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"neutral\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral to negative\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3              neutral\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9              neutral \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       1.00      0.67      0.80         6\n",
      "            neutral       0.67      1.00      0.80         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.80        10\n",
      "          macro avg       0.67      0.67      0.65        10\n",
      "       weighted avg       0.93      0.80      0.84        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 0 negative, 1 neutral, 0 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 7.0, time 11.785068988800049, total tokens 89, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral to negative\",\n",
      "      \"9\": \"neutral\"\n",
      "    }</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3             negative\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9              neutral \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       1.00      0.83      0.91         6\n",
      "            neutral       1.00      1.00      1.00         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.90        10\n",
      "          macro avg       0.75      0.71      0.73        10\n",
      "       weighted avg       1.00      0.90      0.95        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 0 negative, 1 neutral, 1 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 5.0, time 15.83237910270691, total tokens 89, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral to negative\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3             negative\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9              neutral \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       1.00      0.83      0.91         6\n",
      "            neutral       1.00      1.00      1.00         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.90        10\n",
      "          macro avg       0.75      0.71      0.73        10\n",
      "       weighted avg       1.00      0.90      0.95        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 1 negative, 0 neutral, 0 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 4.0, time 19.713567972183228, total tokens 86, total prompts 1\n",
      "{\"0\":\"negative\", \"1\":\"neutral\", \"2\":\"positive\", \"3\":\"negative\", \"4\":\"positive\", \"5\":\"negative\", \"6\":\"negative\", \"7\":\"negative\", \"8\":\"neutral to negative\", \"9\":\"negative\"} \n",
      "\n",
      "     Note: The sentiment analysis is based on the given context of the sentences and may vary depending on additional context or individual interpretation.</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3             negative\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9             negative \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       0.83      0.83      0.83         6\n",
      "            neutral       1.00      0.50      0.67         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.80        10\n",
      "          macro avg       0.71      0.58      0.62        10\n",
      "       weighted avg       0.90      0.80      0.83        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 1 negative, 0 neutral, 1 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 3.0, time 23.78843641281128, total tokens 89, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral to negative\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3             negative\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9              neutral \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       1.00      0.83      0.91         6\n",
      "            neutral       1.00      1.00      1.00         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.90        10\n",
      "          macro avg       0.75      0.71      0.73        10\n",
      "       weighted avg       1.00      0.90      0.95        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 1 negative, 1 neutral, 0 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 3.0, time 27.87007236480713, total tokens 89, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral to negative\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "                     0\n",
      "0             negative\n",
      "1              neutral\n",
      "2             positive\n",
      "3             negative\n",
      "4             positive\n",
      "5             negative\n",
      "6             negative\n",
      "7             negative\n",
      "8  neutral to negative\n",
      "9              neutral \n",
      " 0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           negative       1.00      0.83      0.91         6\n",
      "            neutral       1.00      1.00      1.00         2\n",
      "neutral to negative       0.00      0.00      0.00         0\n",
      "           positive       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.90        10\n",
      "          macro avg       0.75      0.71      0.73        10\n",
      "       weighted avg       1.00      0.90      0.95        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.9454545454545453, 'precision': 1.0, 'recall': 0.9}\n",
      "Testing with: 1 negative, 1 neutral, 1 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 2.0, time 31.814221620559692, total tokens 86, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"negative\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "          0\n",
      "0  negative\n",
      "1   neutral\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n",
      "5  negative\n",
      "6  negative\n",
      "7  negative\n",
      "8  negative\n",
      "9   neutral \n",
      " 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00         6\n",
      "     neutral       1.00      1.00      1.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 1, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "Testing with: 2 negative, 0 neutral, 0 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 2.0, time 35.72451210021973, total tokens 85, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"negative\",\n",
      "      \"9\": \"negative\"\n",
      "     }</s>\n",
      "          0\n",
      "0  negative\n",
      "1   neutral\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n",
      "5  negative\n",
      "6  negative\n",
      "7  negative\n",
      "8  negative\n",
      "9  negative \n",
      " 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       1.00      0.50      0.67         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.95      0.83      0.86        10\n",
      "weighted avg       0.91      0.90      0.89        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 1, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "Testing with: 2 negative, 0 neutral, 1 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 2.0, time 39.70122241973877, total tokens 87, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "          0\n",
      "0  negative\n",
      "1   neutral\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n",
      "5  negative\n",
      "6  negative\n",
      "7  negative\n",
      "8   neutral\n",
      "9   neutral \n",
      " 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91         6\n",
      "     neutral       0.67      1.00      0.80         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.89      0.94      0.90        10\n",
      "weighted avg       0.93      0.90      0.91        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 1, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "Testing with: 2 negative, 1 neutral, 0 positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/sec: 1.0, time 43.69558763504028, total tokens 87, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"neutral\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "          0\n",
      "0  negative\n",
      "1   neutral\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n",
      "5  negative\n",
      "6  negative\n",
      "7  negative\n",
      "8   neutral\n",
      "9   neutral \n",
      " 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91         6\n",
      "     neutral       0.67      1.00      0.80         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.89      0.94      0.90        10\n",
      "weighted avg       0.93      0.90      0.91        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 1, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "Testing with: 2 negative, 1 neutral, 1 positive.\n",
      "tokens/sec: 1.0, time 47.679102420806885, total tokens 86, total prompts 1\n",
      "{\"0\": \"negative\",\n",
      "      \"1\": \"neutral\",\n",
      "      \"2\": \"positive\",\n",
      "      \"3\": \"negative\",\n",
      "      \"4\": \"positive\",\n",
      "      \"5\": \"negative\",\n",
      "      \"6\": \"negative\",\n",
      "      \"7\": \"negative\",\n",
      "      \"8\": \"negative\",\n",
      "      \"9\": \"neutral\"\n",
      "     }</s>\n",
      "          0\n",
      "0  negative\n",
      "1   neutral\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n",
      "5  negative\n",
      "6  negative\n",
      "7  negative\n",
      "8  negative\n",
      "9   neutral \n",
      " 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00         6\n",
      "     neutral       1.00      1.00      1.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 1, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "# model.generation_config.temperature=None\n",
    "\n",
    "params = {\n",
    "    'seed': [42],\n",
    "    'n_neutral_sentences': [0, 1],\n",
    "    'n_positive_sentences': [0, 1],\n",
    "    'n_negative_sentences': [0, 1, 2]\n",
    "}\n",
    "\n",
    "dataset = \"90\"\n",
    "metrics_file = f'PT_context_metrics_{dataset}.csv'\n",
    "if os.path.isfile(metrics_file):\n",
    "    f = open(metrics_file, 'a')\n",
    "else:\n",
    "    f = open(metrics_file, 'w')\n",
    "    f.write('seed,n_neutral_sentences,n_positive_sentences,n_negative_sentences,f1,precision,recall\\n')\n",
    "\n",
    "best_model = []\n",
    "best_f1 = 0.0\n",
    "best_comb, best_results = None, None\n",
    "for comb in list(ParameterGrid(params)):\n",
    "    print(f\"Testing with: {comb['n_negative_sentences']} negative, {comb['n_neutral_sentences']} neutral, {comb['n_positive_sentences']} positive.\")\n",
    "    train = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/train_{dataset}.csv\")\n",
    "    test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/test_{dataset}.csv\")\n",
    "    # randomly selecting context sentences in json format\n",
    "    train_sentences = train_sentence_selection(train,\n",
    "                                               comb['n_neutral_sentences'],\n",
    "                                               comb['n_positive_sentences'],\n",
    "                                               comb['n_negative_sentences'],\n",
    "                                               \"PT\",\n",
    "                                               comb['seed'])\n",
    "    json_train_sentences = train_sentences[\"language\"].to_json()\n",
    "    json_train_labels = train_sentences[\"PT_label\"].to_json()\n",
    "\n",
    "    # converting test sentences to json format\n",
    "    json_test_sentences = test[\"language\"].to_json()\n",
    "    sentence_count = test['language'].size\n",
    "\n",
    "    # creating context prompt\n",
    "\n",
    "\n",
    "    USER_PROMPT_1 = f\"\"\"\n",
    "    "Doctors write lots of clinical notes about you.\n",
    "    "Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\n",
    "    "For each sentence, how do you feel reading this description of you?\n",
    "    "Answer the question by assigning a sentiment score of negative, neutral, or positive for each sentence.\n",
    "    "Output only your sentiment scores in JSON format:\n",
    "    "{json_test_sentences}\n",
    "                        \"\"\"\n",
    "    \n",
    "    ASSISTANT_PROMPT = f\"{json_train_labels}\"\n",
    "\n",
    "    USER_PROMPT_2 = f\"\"\"Complete the same task with each of these sentences and output your sentiment scores (negative, neutral, or positive) in JSON format:\\n{json_test_sentences} \n",
    "                        \"\"\"\n",
    "\n",
    "    PROMPT = [f\"\"\"\n",
    "    <s>[INST]{USER_PROMPT_1}[/INST]\n",
    "    {ASSISTANT_PROMPT}</s>\n",
    "    [INST]{USER_PROMPT_2}[/INST]\n",
    "    \"\"\"]\n",
    "        \n",
    "    with accelerator.split_between_processes(PROMPT) as prompts:\n",
    "        # store output of generations in dict\n",
    "        results=dict(outputs=[], num_tokens=0)\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "        for prompt in prompts:\n",
    "            prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=800, do_sample=True, temperature=0.001)[0]\n",
    "\n",
    "        # remove prompt from output\n",
    "            output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "            results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "            results[\"num_tokens\"] += len(output_tokenized)\n",
    "\n",
    "        results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "    results_gathered=gather_object(results)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        timediff=time.time()-start\n",
    "        num_tokens=sum([r[\"num_tokens\"] for r in results_gathered ])\n",
    "\n",
    "        print(f\"tokens/sec: {num_tokens//timediff}, time {timediff}, total tokens {num_tokens}, total prompts {len(PROMPT)}\")\n",
    "\n",
    "    for o in results_gathered[0]['outputs']:\n",
    "        print(o)\n",
    "    \n",
    "    # running chat completion\n",
    "    output = parse_json_garbage(o)\n",
    "    try:\n",
    "        output = pd.DataFrame.from_dict(output).transpose()\n",
    "    except ValueError:\n",
    "        output = pd.DataFrame.from_dict(output, orient='index')\n",
    "    column_name = list(output.columns.values)\n",
    "    column_name = column_name[0]\n",
    "    print(output, '\\n', column_name)\n",
    "\n",
    "    \n",
    "    # computing metrics\n",
    "    results = compute_metrics(output, test[\"PT_label\"])\n",
    "#     except ValueError:\n",
    "#         output = output.iloc[0:23]\n",
    "#         results = compute_metrics(output, test[\"MD_label\"])\n",
    "    # saving results to metrics sheet\n",
    "    v = [comb['seed'], comb['n_neutral_sentences'], comb['n_positive_sentences'], comb['n_negative_sentences'],\n",
    "         results['f1'], results['precision'], results['recall']]\n",
    "    f.write(','.join([str(el) for el in v]) + '\\n')\n",
    "\n",
    "    if results['f1'] > best_f1:\n",
    "        best_f1 = results['f1']\n",
    "        best_comb = comb\n",
    "        best_results = results\n",
    "        best_CLI_sentences = train_sentences\n",
    "        best_CLI_sentences.to_csv(f'best_CLI_sentences_{dataset}.csv', index=False)\n",
    "        error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], output[column_name]], axis=1)\n",
    "        error_analysis = error_analysis.rename(columns={column_name:\"pred\", \"PT_label\":\"true\"})\n",
    "        mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "        error_analysis = error_analysis[~ mask]\n",
    "        error_analysis.to_csv(f'error_analysis_{dataset}.csv', index=False)\n",
    "    print('-' * 100)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    if best_comb is not None:\n",
    "        print(f'Best combination of context sentences: {best_comb}')\n",
    "        print('\\n')\n",
    "        print(f'Best results: {best_results}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
