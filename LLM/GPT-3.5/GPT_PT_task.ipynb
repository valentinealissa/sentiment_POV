{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=#setkeyhere\n",
    "import evaluate\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import evaluate\n",
    "\n",
    "client = AzureOpenAI(azure_endpoint='https://oai-cbipm-01.openai.azure.com/',\n",
    "                     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                     api_version=\"2023-12-01-preview\")  # 2023-12-01-preview <- highest version number as of 1/12/23\n",
    "\n",
    "deployment = \"Deployment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metrics function\n",
    "def compute_metrics(predictions, references, labels=None, pos_label=1, average=\"weighted\", sample_weight=None, zero_division='warn'):\n",
    "        f1 = f1_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight\n",
    "        )\n",
    "        p = precision_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight,\n",
    "            zero_division=zero_division\n",
    "        )\n",
    "        r = recall_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight,\n",
    "            zero_division=zero_division\n",
    "        )\n",
    "        c = classification_report(\n",
    "            references, predictions, labels=labels\n",
    "        )\n",
    "        print(c)\n",
    "        return {\"f1\": float(f1) if f1.size == 1 else f1,\n",
    "                \"precision\": float(p) if p.size == 1 else p,\n",
    "                \"recall\": float(r) if r.size == 1 else r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training sentence selection function\n",
    "def train_sentence_selection(df, n_neutral, n_positive, n_negative, label, seed):\n",
    "    # saving column name given PT or MD label\n",
    "    label = f\"{label}_label\"\n",
    "    # randomly selecting 1-2 sentences per label\n",
    "    neutral_sentences = df[df[label] == \"neutral\"].sample(n_neutral, replace=False, random_state=seed)\n",
    "    positive_sentences = df[df[label] == \"positive\"].sample(n_positive, replace=False, random_state=seed)\n",
    "    negative_sentences = df[df[label] == \"negative\"].sample(n_negative, replace=False, random_state=seed)\n",
    "    all_sentences = pd.concat([neutral_sentences,\n",
    "                               positive_sentences,\n",
    "                               negative_sentences], ignore_index = True)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"0\": \"negative\", \"1\": \"neutral\", \"2\": \"negative\", \"3\": \"negative\", \"4\": \"negative\", \"5\": \"negative\", \"6\": \"neutral\", \"7\": \"positive\", \"8\": \"positive\", \"9\": \"negative\", \"10\": \"negative\", \"11\": \"negative\", \"12\": \"negative\", \"13\": \"negative\", \"14\": \"negative\"}\n",
      "negative\n",
      " neutral\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      " neutral\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_49035/649420051.py:22: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"best_CLI_sentences_90.csv\")\n",
    "test = pd.read_csv(\"../data/validation_sentences.csv\")\n",
    "json_train_sentences = train[\"language\"].to_json()\n",
    "json_train_labels = train[\"PT_label\"].to_json()\n",
    "json_test_sentences = test[\"language\"].to_json()\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a patient.\"},\n",
    "            {\"role\": \"user\", \"content\": \"As a patient at a medical center, medical doctors write lots of clinical notes about you.\\n\"\n",
    "                                        \"Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\\n\"\n",
    "                                        \"For each sentence, how do you feel reading this description of you?\\n\"\n",
    "                                        \"Please assign a sentiment score of negative, neutral, or positive for each sentence.\\n\"\n",
    "                                        \"Below are some example sentences in JSON format:\\n\"\n",
    "                                        f\"{json_train_sentences}\"\n",
    "                                        \"Please provide your answer in JSON format.\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{json_train_labels}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Complete the same task with each of these sentences:\\n{json_test_sentences}\"}]\n",
    "\n",
    "response = client.chat.completions.create(model=deployment, messages=messages, temperature=0, seed=42)\n",
    "\n",
    "output = response.choices[0].message.content\n",
    "print(output)\n",
    "predictions = pd.read_json(output, orient=\"index\")\n",
    "# results = compute_metrics(predictions[0], test[\"PT_label\"])\n",
    "# print(results)\n",
    "# error_analysis = pd.concat([test[[\"language\", \"MD_PT_label\", \"PT_label\"]], predictions], axis=1)\n",
    "# error_analysis = error_analysis.rename(columns={0:\"pred\", \"PT_label\":\"true\"})\n",
    "# mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "# error_analysis = error_analysis[~ mask]\n",
    "# error_analysis.to_csv('error_analysis_validation_NA.csv', index=False)\n",
    "print(predictions[0].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"0\": \"neutral\", \"1\": \"neutral\", \"2\": \"negative\", \"3\": \"negative\", \"4\": \"negative\", \"5\": \"negative\", \"6\": \"neutral\", \"7\": \"positive\", \"8\": \"positive\", \"9\": \"negative\", \"10\": \"negative\", \"11\": \"negative\", \"12\": \"negative\", \"13\": \"negative\", \"14\": \"negative\"}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.89      0.84         9\n",
      "     neutral       0.67      0.50      0.57         4\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.82      0.80      0.80        15\n",
      "weighted avg       0.79      0.80      0.79        15\n",
      "\n",
      "{'f1': 0.7909774436090227, 'precision': 0.7911111111111111, 'recall': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_31101/2847863793.py:17: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/validation_sentences.csv\")\n",
    "json_train_sentences = train[\"language\"].to_json()\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a patient.\"},\n",
    "            {\"role\": \"user\", \"content\": \"As a patient at a medical center, medical doctors write lots of clinical notes about you.\\n\"\n",
    "                                        \"Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\\n\"\n",
    "                                        \"For each sentence, how do you feel reading this description of you?\\n\"\n",
    "                                        \"Please assign a sentiment score of negative, neutral, or positive for each sentence.\\n\"\n",
    "                                        \"Below are some  sentences in JSON format:\\n\"\n",
    "                                        f\"{json_train_sentences}\"\n",
    "                                        \"Please provide your answer in JSON format.\"}]\n",
    "\n",
    "response = client.chat.completions.create(model=deployment, messages=messages, temperature=0, seed=42)\n",
    "\n",
    "output = response.choices[0].message.content\n",
    "print(output)\n",
    "predictions = pd.read_json(output, orient=\"index\")\n",
    "results = compute_metrics(predictions[0], train[\"PT_label\"])\n",
    "print(results)\n",
    "error_analysis = pd.concat([train[[\"language\", \"MD_PT_label\", \"PT_label\"]], predictions], axis=1)\n",
    "error_analysis = error_analysis.rename(columns={0:\"pred\", \"PT_label\":\"true\"})\n",
    "mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "error_analysis = error_analysis[~ mask]\n",
    "error_analysis.to_csv('error_analysis_zero_shot_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.51      0.67      0.57        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.7138461538461538, 'precision': 0.6476190476190475, 'recall': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.58      0.67      0.62        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.7142857142857142, 'precision': 0.65, 'recall': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.58      0.67      0.62        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.7142857142857142, 'precision': 0.65, 'recall': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.58      0.67      0.62        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 0, 'n_neutral_sentences': 0, 'n_positive_sentences': 1, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.7142857142857142, 'precision': 0.65, 'recall': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       1.00      0.50      0.67         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.95      0.83      0.86        10\n",
      "weighted avg       0.91      0.90      0.89        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.58      0.67      0.62        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.58      0.67      0.62        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valena17/azure_gpt/azure_gpt_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         6\n",
      "     neutral       0.00      0.00      0.00         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.58      0.67      0.62        10\n",
      "weighted avg       0.65      0.80      0.71        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       1.00      0.50      0.67         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.95      0.83      0.86        10\n",
      "weighted avg       0.91      0.90      0.89        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       1.00      0.50      0.67         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.95      0.83      0.86        10\n",
      "weighted avg       0.91      0.90      0.89        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       1.00      0.50      0.67         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.95      0.83      0.86        10\n",
      "weighted avg       0.91      0.90      0.89        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      1.00      0.92         6\n",
      "     neutral       1.00      0.50      0.67         2\n",
      "    positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.95      0.83      0.86        10\n",
      "weighted avg       0.91      0.90      0.89        10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Best combination of context sentences: {'n_negative_sentences': 1, 'n_neutral_sentences': 0, 'n_positive_sentences': 0, 'seed': 42}\n",
      "\n",
      "\n",
      "Best results: {'f1': 0.8871794871794872, 'precision': 0.9142857142857143, 'recall': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_1161/384148244.py:53: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  predictions = pd.read_json(output, orient=\"index\")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (for best configuration selection)\n",
    "params = {\n",
    "    'seed': [42],\n",
    "    'n_neutral_sentences': [0, 1],\n",
    "    'n_positive_sentences': [0, 1],\n",
    "    'n_negative_sentences': [0, 1, 2]\n",
    "}\n",
    "\n",
    "dataset = \"90\"\n",
    "metrics_file = f'PT_context_metrics_{dataset}.csv'\n",
    "if os.path.isfile(metrics_file):\n",
    "    f = open(metrics_file, 'a')\n",
    "else:\n",
    "    f = open(metrics_file, 'w')\n",
    "    f.write('seed,n_neutral_sentences,n_positive_sentences,n_negative_sentences,f1,precision,recall\\n')\n",
    "\n",
    "best_model = []\n",
    "best_f1 = 0.0\n",
    "best_comb, best_results = None, None\n",
    "for comb in list(ParameterGrid(params)):\n",
    "    train = pd.read_csv(f\"../data/train_{dataset}.csv\")\n",
    "    test = pd.read_csv(f\"../data/test_{dataset}.csv\")\n",
    "    # randomly selecting context sentences in json format\n",
    "    train_sentences = train_sentence_selection(train,\n",
    "                                               comb['n_neutral_sentences'],\n",
    "                                               comb['n_positive_sentences'],\n",
    "                                               comb['n_negative_sentences'],\n",
    "                                               \"PT\",\n",
    "                                               comb['seed'])\n",
    "    train_sentences.to_csv(f'CLI_sentences_{dataset}.csv', index=False)\n",
    "    json_train_sentences = train_sentences[\"language\"].to_json()\n",
    "    json_train_labels = train_sentences[\"PT_label\"].to_json()\n",
    "\n",
    "    # converting test sentences to json format\n",
    "    json_test_sentences = test[\"language\"].to_json()\n",
    "\n",
    "    # creating context prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a patient.\"},\n",
    "                {\"role\": \"user\", \"content\": \"As a patient at a medical center, medical doctors write lots of clinical notes about you.\\n\"\n",
    "                                            \"Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\\n\"\n",
    "                                            \"For each sentence, how do you feel reading this description of you?\\n\"\n",
    "                                            \"Please assign a sentiment score of negative, neutral, or positive for each sentence.\\n\"\n",
    "                                            \"Below are some example sentences in JSON format:\\n\"\n",
    "                                            f\"{json_train_sentences}\"\n",
    "                                            \"Please provide your answer in JSON format.\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{json_train_labels}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Complete the same task with each of these sentences:\\n{json_test_sentences}\"}]\n",
    "\n",
    "    # running chat completion\n",
    "    response = client.chat.completions.create(model=deployment, messages=messages, temperature=0, seed=comb['seed'])\n",
    "    # saving response to json format\n",
    "    output = response.choices[0].message.content\n",
    "    predictions = pd.read_json(output, orient=\"index\")\n",
    "    # computing metrics\n",
    "    results = compute_metrics(predictions[0], test[\"PT_label\"])\n",
    "    # saving results to metrics sheet\n",
    "    v = [comb['seed'], comb['n_neutral_sentences'], comb['n_positive_sentences'], comb['n_negative_sentences'],\n",
    "         results['f1'], results['precision'], results['recall']]\n",
    "    f.write(','.join([str(el) for el in v]) + '\\n')\n",
    "\n",
    "    if results['f1'] > best_f1:\n",
    "        best_f1 = results['f1']\n",
    "        best_comb = comb\n",
    "        best_results = results\n",
    "        best_CLI_sentences = train_sentences\n",
    "        best_CLI_sentences.to_csv(f'best_CLI_sentences_{dataset}.csv', index=False)\n",
    "        error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], predictions], axis=1)\n",
    "        error_analysis = error_analysis.rename(columns={0:\"pred\", \"PT_label\":\"true\"})\n",
    "        mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "        error_analysis = error_analysis[~ mask]\n",
    "        error_analysis.to_csv(f'error_analysis_{dataset}.csv', index=False)\n",
    "    print('-' * 100)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    if best_comb is not None:\n",
    "        print(f'Best combination of context sentences: {best_comb}')\n",
    "        print('\\n')\n",
    "        print(f'Best results: {best_results}')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
