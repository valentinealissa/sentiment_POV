{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ff439367d44bdc916cb0bcc6957d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from statistics import mean\n",
    "import transformers\n",
    "import torch, time, json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:50\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# load a base model and tokenizer in current directory\n",
    "access_token = #your token here\n",
    "model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token, cache_dir='./')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token, cache_dir='./',\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, references, labels=None, pos_label=1, average=\"weighted\", sample_weight=None, zero_division='warn'):\n",
    "        f1 = f1_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight\n",
    "        )\n",
    "        p = precision_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight,\n",
    "            zero_division=zero_division\n",
    "        )\n",
    "        r = recall_score(\n",
    "            references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight,\n",
    "            zero_division=zero_division\n",
    "        )\n",
    "        c = classification_report(\n",
    "            references, predictions, labels=labels\n",
    "        )\n",
    "        print(c)\n",
    "        return {\"f1\": float(f1) if f1.size == 1 else f1,\n",
    "                \"precision\": float(p) if p.size == 1 else p,\n",
    "                \"recall\": float(r) if r.size == 1 else r}\n",
    "\n",
    "def train_sentence_selection(df, n_neutral, n_positive, n_negative, label, seed):\n",
    "    # saving column name given PT or MD label\n",
    "    label = f\"{label}_label\"\n",
    "    # randomly selecting 1-2 sentences per label\n",
    "    neutral_sentences = df[df[label] == \"neutral\"].sample(n_neutral, replace=False, random_state=seed)\n",
    "    positive_sentences = df[df[label] == \"positive\"].sample(n_positive, replace=False, random_state=seed)\n",
    "    negative_sentences = df[df[label] == \"negative\"].sample(n_negative, replace=False, random_state=seed)\n",
    "    all_sentences = pd.concat([neutral_sentences,\n",
    "                               positive_sentences,\n",
    "                               negative_sentences], ignore_index = True)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot approach\n",
    "Below is code to evaluate the model with a zero-shot approach.\n",
    "Update the right dataset and file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90         9\n",
      "     neutral       1.00      0.25      0.40         4\n",
      "    positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.83      0.75      0.70        15\n",
      "weighted avg       0.85      0.80      0.75        15\n",
      "\n",
      "{'f1': 0.7533333333333333, 'precision': 0.8464646464646465, 'recall': 0.8}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## zero shot approach\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "\n",
    "dataset = \"80\" #example, evaluating the model on sentences with at least 80% agreement\n",
    "\n",
    "test = pd.read_csv(f\"yourpath/data/test_{dataset}.csv\")\n",
    "\n",
    "# converting test sentences to json format\n",
    "json_test_sentences = test[\"language\"].to_json()\n",
    "\n",
    "# creating context prompt\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a patient at a medical center.<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "PROMPTS = []\n",
    "\n",
    "for sentence in test[\"language\"]:\n",
    "    \n",
    "    USER_PROMPT_1 = f\"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Doctors write lots of clinical notes about you.\n",
    "    Your task is to analyze the sentiment of a sentence your doctor wrote about you.\n",
    "    For each sentence, how do you feel reading this description of you?\n",
    "    Please assign a sentiment score of negative, neutral, or positive for the sentence.\n",
    "    Only output your sentiment score in JSON format for this sentence:\n",
    "    {sentence}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT = [f\"\"\"\n",
    "    {SYSTEM_PROMPT}\n",
    "    {USER_PROMPT_1}\n",
    "    \"\"\"]\n",
    "    PROMPTS.append(PROMPT)\n",
    "\n",
    "\n",
    "with accelerator.split_between_processes(PROMPTS) as prompts:\n",
    "    # store output of generations in dict   \n",
    "    results=dict(outputs=[])\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=10, do_sample=True, temperature=0.001, pad_token_id=tokenizer.eos_token_id)[0]\n",
    "\n",
    "        # remove prompt from output\n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "        \n",
    "    results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "results = results_gathered[0]['outputs']\n",
    "parsed_results = []\n",
    "for label in results:\n",
    "    label = label.strip().replace('<|eot_id|>', '')\n",
    "    parsed_results.append(label)\n",
    "    \n",
    "parsed_data = [json.loads(item) for item in parsed_results]\n",
    "df = pd.DataFrame(parsed_data)\n",
    "df.columns = ['Model_label']\n",
    "\n",
    "results = compute_metrics(df['Model_label'], test[\"PT_label\"])\n",
    "print(results)\n",
    "\n",
    "error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], df['Model_label']], axis=1)\n",
    "error_analysis = error_analysis.rename(columns={'Model_label':\"pred\", \"PT_label\":\"true\"})\n",
    "mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "error_analysis = error_analysis[~ mask]\n",
    "error_analysis.to_csv(f'PT_error_analysis_zero_shot_validation_{dataset}.csv', index=False)\n",
    "print('-' * 100)\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICL approach\n",
    "Below is code to evaluate the model with an ICL approach.\n",
    "Update the right dataset and file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         9\n",
      "     neutral       0.00      0.00      0.00         4\n",
      "    positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.47      0.67      0.55        15\n",
      "weighted avg       0.54      0.73      0.62        15\n",
      "\n",
      "{'f1': 0.6209523809523809, 'precision': 0.538888888888889, 'recall': 0.7333333333333333}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/hpc/packages/minerva-centos7/py_packages/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "## CLI test\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "\n",
    "dataset = \"90\"\n",
    "\n",
    "\n",
    "train = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/PT_task/LLAMA3/3.1/best_CLI_sentences_{dataset}.csv\")\n",
    "test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/validation_sentences.csv\")\n",
    "\n",
    "#context sentences in json format\n",
    "json_train_sentences = train[\"language\"].to_json()\n",
    "json_train_labels = train[\"PT_label\"].to_json()\n",
    "\n",
    "# converting test sentences to json format\n",
    "json_test_sentences = test[\"language\"].to_json()\n",
    "\n",
    "# creating context prompt\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a patient at a medical center.<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "USER_PROMPT_1 = f\"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Doctors write lots of clinical notes about you.\n",
    "    Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\n",
    "    For each sentence, how do you feel reading this description of you?\n",
    "    Answer the question by assigning a sentiment score of negative, neutral, or positive for the sentence.\n",
    "    Output your anser in JSON format. Don’t add explanation beyond the JSON.\n",
    "    Below are some example sentences in JSON format:\n",
    "    {json_train_sentences}<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "ASSISTANT_PROMPT = f\"\"\"\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    f{json_train_labels}<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "PROMPTS = []\n",
    "\n",
    "for sentence in test[\"language\"]:\n",
    "    \n",
    "    USER_PROMPT_2 = f\"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Complete the same task with this sentence and only return your sentiment score in JSON format:\\n\"{sentence}\"<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT = [f\"\"\"\n",
    "    {SYSTEM_PROMPT}\n",
    "    {USER_PROMPT_1}\n",
    "    {ASSISTANT_PROMPT}\n",
    "    {USER_PROMPT_2}\n",
    "    \"\"\"]\n",
    "    PROMPTS.append(PROMPT)\n",
    "\n",
    "\n",
    "with accelerator.split_between_processes(PROMPTS) as prompts:\n",
    "    # store output of generations in dict   \n",
    "    results=dict(outputs=[])\n",
    "\n",
    "    # have each GPU do inference, prompt by prompt\n",
    "    for prompt in prompts:\n",
    "        prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=10, do_sample=True, temperature=0.001, pad_token_id=tokenizer.eos_token_id)[0]\n",
    "\n",
    "        # remove prompt from output\n",
    "        output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "        # store outputs and number of tokens in result{}\n",
    "        results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "#         results[\"num_tokens\"].append(len(output_tokenized))\n",
    "\n",
    "        \n",
    "    results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "results_gathered=gather_object(results)\n",
    "# print(results_gathered)\n",
    "\n",
    "results = results_gathered[0]['outputs']\n",
    "parsed_results = []\n",
    "for label in results:\n",
    "    label = label.strip().replace('<|eot_id|>', '')\n",
    "    parsed_results.append(label)\n",
    "    \n",
    "parsed_data = [json.loads(item) for item in parsed_results]\n",
    "df = pd.DataFrame(parsed_data)\n",
    "df.columns = ['Model_label']\n",
    "\n",
    "results = compute_metrics(df['Model_label'], test[\"PT_label\"])\n",
    "print(results)\n",
    "\n",
    "error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], df['Model_label']], axis=1)\n",
    "# print(error_analysis)\n",
    "error_analysis = error_analysis.rename(columns={'Model_label':\"pred\", \"PT_label\":\"true\"})\n",
    "mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "error_analysis = error_analysis[~ mask]\n",
    "error_analysis.to_csv(f'error_analysis_test_{dataset}.csv', index=False)\n",
    "print('-' * 100)\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI experiment\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "start=time.time()\n",
    "# model.generation_config.temperature=None\n",
    "\n",
    "params = {\n",
    "    'seed': [42],\n",
    "    'n_neutral_sentences': [0, 1, 2, 3, 4],\n",
    "    'n_positive_sentences': [0, 1, 2, 3],\n",
    "    'n_negative_sentences': [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "dataset = \"60\"\n",
    "metrics_file = f'PT_context_metrics_{dataset}.csv'\n",
    "if os.path.isfile(metrics_file):\n",
    "    f = open(metrics_file, 'a')\n",
    "else:\n",
    "    f = open(metrics_file, 'w')\n",
    "    f.write('seed,n_neutral_sentences,n_positive_sentences,n_negative_sentences,f1,precision,recall\\n')\n",
    "\n",
    "best_model = []\n",
    "best_f1 = 0.0\n",
    "best_comb, best_results = None, None\n",
    "for comb in list(ParameterGrid(params)):\n",
    "    print(f\"Testing with: {comb['n_negative_sentences']} negative, {comb['n_neutral_sentences']} neutral, {comb['n_positive_sentences']} positive.\")\n",
    "    train = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/train_{dataset}.csv\")\n",
    "    test = pd.read_csv(f\"/sc/arion/projects/mscic1/psych_nlp/sentiment_analysis/data/test_{dataset}.csv\")\n",
    "    # randomly selecting context sentences in json format\n",
    "    train_sentences = train_sentence_selection(train,\n",
    "                                               comb['n_neutral_sentences'],\n",
    "                                               comb['n_positive_sentences'],\n",
    "                                               comb['n_negative_sentences'],\n",
    "                                               \"PT\",\n",
    "                                               comb['seed'])\n",
    "    json_train_sentences = train_sentences[\"language\"].to_json()\n",
    "    json_train_labels = train_sentences[\"PT_label\"].to_json()\n",
    "\n",
    "    # converting test sentences to json format\n",
    "    json_test_sentences = test[\"language\"].to_json()\n",
    "    sentence_count = test['language'].size\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a patient at a medical center.<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    USER_PROMPT_1 = f\"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Doctors write lots of clinical notes about you.\n",
    "    Your task is to analyze the sentiment of a series of sentences your doctor wrote about you.\n",
    "    For each sentence, how do you feel reading this description of you?\n",
    "    Answer the question by assigning a sentiment score of negative, neutral, or positive for the sentence.\n",
    "    Output your anser in JSON format. Don’t add explanation beyond the JSON.\n",
    "    Below are some example sentences in JSON format:\n",
    "    {json_train_sentences}<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    ASSISTANT_PROMPT = f\"\"\"\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    f{json_train_labels}<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPTS = []\n",
    "\n",
    "    for sentence in test[\"language\"]:\n",
    "\n",
    "        USER_PROMPT_2 = f\"\"\"\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Complete the same task with this sentence and only return your sentiment score in JSON format:\\n\"{sentence}\"<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "\n",
    "        PROMPT = [f\"\"\"\n",
    "        {SYSTEM_PROMPT}\n",
    "        {USER_PROMPT_1}\n",
    "        {ASSISTANT_PROMPT}\n",
    "        {USER_PROMPT_2}\n",
    "        \"\"\"]\n",
    "        PROMPTS.append(PROMPT)\n",
    "\n",
    "\n",
    "    with accelerator.split_between_processes(PROMPTS) as prompts:\n",
    "        # store output of generations in dict   \n",
    "        results=dict(outputs=[])\n",
    "\n",
    "        # have each GPU do inference, prompt by prompt\n",
    "        for prompt in prompts:\n",
    "            prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=10, do_sample=True, temperature=0.001, pad_token_id=tokenizer.eos_token_id)[0]\n",
    "\n",
    "            # remove prompt from output\n",
    "            output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "            # store outputs and number of tokens in result{}\n",
    "            results[\"outputs\"].append( tokenizer.decode(output_tokenized) )\n",
    "\n",
    "\n",
    "        results=[ results ] # transform to list, otherwise gather_object() will not collect correctly\n",
    "\n",
    "    # collect results from all the GPUs\n",
    "    results_gathered=gather_object(results)\n",
    "    results = results_gathered[0]['outputs']\n",
    "    parsed_results = []\n",
    "    for label in results:\n",
    "        label = label.strip().replace('<|eot_id|>', '')\n",
    "        parsed_results.append(label)\n",
    "\n",
    "    parsed_data = [json.loads(item) for item in parsed_results]\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    df.columns = ['Model_label']\n",
    "\n",
    "    results = compute_metrics(df['Model_label'], test[\"PT_label\"])\n",
    "    print(results)\n",
    "\n",
    "    \n",
    "    # saving results to metrics sheet\n",
    "    v = [comb['seed'], comb['n_neutral_sentences'], comb['n_positive_sentences'], comb['n_negative_sentences'],\n",
    "         results['f1'], results['precision'], results['recall']]\n",
    "    f.write(','.join([str(el) for el in v]) + '\\n')\n",
    "\n",
    "    if results['f1'] > best_f1:\n",
    "        best_f1 = results['f1']\n",
    "        best_comb = comb\n",
    "        best_results = results\n",
    "        best_CLI_sentences = train_sentences\n",
    "        best_CLI_sentences.to_csv(f'best_CLI_sentences_{dataset}.csv', index=False)\n",
    "        error_analysis = pd.concat([test[[\"idx\", \"language\", \"MD_PT_label\", \"PT_label\"]], df['Model_label']], axis=1)\n",
    "    #   print(error_analysis)\n",
    "        error_analysis = error_analysis.rename(columns={'Model_label':\"pred\", \"PT_label\":\"true\"})\n",
    "        mask = error_analysis[\"pred\"] == error_analysis[\"true\"]\n",
    "        error_analysis = error_analysis[~ mask]\n",
    "        error_analysis.to_csv(f'error_analysis_validation_ICL_{dataset}.csv', index=False)\n",
    "    print('-' * 100)\n",
    "    print('\\n\\n')\n",
    "    print('-' * 100)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    if best_comb is not None:\n",
    "        print(f'Best combination of context sentences: {best_comb}')\n",
    "        print('\\n')\n",
    "        print(f'Best results: {best_results}')\n",
    "    torch.cuda.empty_cache()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
